{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Support recovery on structured data (2D)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import ndimage\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import FeatureAgglomeration\n\nfrom hidimstat.stat_tools import zscore_from_sf_and_cdf\nfrom hidimstat.desparsified_lasso import desparsified_lasso_confint\nfrom hidimstat.stat_tools import sf_from_cb, cdf_from_cb\nfrom hidimstat.clustered_inference import clustered_inference\nfrom hidimstat.ensemble_clustered_inference import ensemble_clustered_inference\nfrom hidimstat.stat_tools import zscore_from_sf\nfrom hidimstat.noise_std import empirical_snr\n\n\ndef simulation_2D(n_samples, shape, roi_size, sigma, smooth_X, seed=0):\n\n    np.random.seed(seed)\n    w = np.zeros(shape + (5,))\n    w[0:roi_size, 0:roi_size, 0] = 1.0\n    w[-roi_size:, -roi_size:, 1] = 1.0\n    w[0:roi_size, -roi_size:, 2] = 1.0\n    w[-roi_size:, 0:roi_size, 3] = 1.0\n    beta = w.sum(-1).ravel()\n\n    X_ = np.random.randn(n_samples, shape[0], shape[1])\n    X_init = []\n\n    for i in np.arange(n_samples):\n        Xi = ndimage.filters.gaussian_filter(X_[i], smooth_X)\n        X_init.append(Xi.ravel())\n\n    X_init = np.asarray(X_init)\n\n    X_ = X_init.reshape((n_samples, shape[0], shape[1]))\n\n    epsilon = sigma * np.random.randn(n_samples)\n    y = np.dot(X_init, beta) + epsilon\n\n    return X_init, y, beta, epsilon\n\n\ndef weight_map_2D_extended(shape, roi_size, delta):\n\n    roi_size_extended = roi_size + delta\n\n    w = np.zeros(shape + (5,))\n    w[0:roi_size, 0:roi_size, 0] = 0.5\n    w[-roi_size:, -roi_size:, 1] = 0.5\n    w[0:roi_size, -roi_size:, 2] = 0.5\n    w[-roi_size:, 0:roi_size, 3] = 0.5\n    w[0:roi_size_extended, 0:roi_size_extended, 0] += 0.5\n    w[-roi_size_extended:, -roi_size_extended:, 1] += 0.5\n    w[0:roi_size_extended, -roi_size_extended:, 2] += 0.5\n    w[-roi_size_extended:, 0:roi_size_extended, 3] += 0.5\n\n    for i in range(roi_size_extended):\n        for j in range(roi_size_extended):\n            if (i - roi_size) + (j - roi_size) >= delta:\n                w[i, j, 0] = 0\n                w[-i-1, -j-1, 1] = 0\n                w[i, -j-1, 2] = 0\n                w[-i-1, j, 3] = 0\n\n    beta_extended = w.sum(-1).ravel()\n\n    return beta_extended\n\n\ndef add_one_subplot(ax, map, title):\n\n    if map is not None:\n        im = ax.imshow(map)\n        im.set_clim(-1, 1)\n        ax.tick_params(\n            axis='both',\n            which='both',\n            bottom=False,\n            top=False,\n            left=False,\n            labelbottom=False,\n            labelleft=False)\n        ax.set_title(title)\n    else:\n        ax.axis('off')\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n\ndef plot(maps, titles):\n\n    fig, axes = \\\n        plt.subplots(3, 2, figsize=(4, 6))\n\n    for i in range(3):\n        for j in range(2):\n            k = i * 2 + j\n            add_one_subplot(axes[i][j], maps[k], titles[k])\n\n    fig.tight_layout()\n\n    figname = f'figures/simu_2D.png'\n    plt.savefig(figname)\n    print(f'Save figure to {figname}')\n\n    plt.show()\n\n\ndef main():\n\n    # simulation parameters\n    seed = 0\n    n_samples = 100\n    shape = (40, 40)\n    n_features = shape[1] * shape[0]\n    roi_size = 4\n    sigma = 2.0\n    smooth_X = 1.0\n\n    # hyper-parameters\n    n_clusters = 200\n\n    # inference parameters\n    fwer_target = 0.1\n    delta = 6\n    # delta = 6 is the tolerance parameter corresponding to\n    # n_clusters = 200 for this particular scenario.\n    # To compute it, one must compute the largest cluster diameter.\n\n    # computation parameter\n    n_jobs = 1\n\n    # computing the thresholds for feature selection\n    correction_no_cluster = 1. / n_features\n    correction_cluster = 1. / n_clusters\n    thr_c = zscore_from_sf((fwer_target / 2) * correction_cluster)\n    thr_nc = zscore_from_sf((fwer_target / 2) * correction_no_cluster)\n\n    X_init, y, beta, epsilon = \\\n        simulation_2D(n_samples, shape, roi_size, sigma, smooth_X, seed)\n\n    empirical_snr(X_init, y, beta, epsilon)\n\n    beta_extended = weight_map_2D_extended(shape, roi_size, delta)\n\n    # desparsified lasso\n    beta_hat, cb_min, cb_max = \\\n        desparsified_lasso_confint(X_init, y, n_jobs=n_jobs)\n    sf, sf_corr = sf_from_cb(cb_min, cb_max)\n    cdf, cdf_corr = cdf_from_cb(cb_min, cb_max)\n    zscore = zscore_from_sf_and_cdf(sf, cdf)\n    selected_dl = zscore > thr_nc\n    selected_dl = np.logical_or(sf_corr < fwer_target / 2,\n                                cdf_corr < fwer_target / 2)\n\n    # clustered desparsified lasso (CluDL)\n    connectivity = image.grid_to_graph(n_x=shape[0],\n                                       n_y=shape[1])\n    ward = FeatureAgglomeration(n_clusters=n_clusters,\n                                connectivity=connectivity,\n                                linkage='ward')\n    sf, sf_corr, cdf, cdf_corr = \\\n        clustered_inference(X_init, y, ward, n_clusters)\n    zscore = zscore_from_sf_and_cdf(sf, cdf)\n    selected_cdl = zscore > thr_c\n    selected_cdl = np.logical_or(sf_corr < fwer_target / 2,\n                                 cdf_corr < fwer_target / 2)\n\n    # ensemble of clustered desparsified lasso (EnCluDL)\n    sf, sf_corr, cdf, cdf_corr = \\\n        ensemble_clustered_inference(X_init, y, ward,\n                                     n_clusters, train_size=0.1)\n    zscore = zscore_from_sf_and_cdf(sf, cdf)\n    selected_ecdl = zscore > thr_c\n    selected_ecdl = np.logical_or(sf_corr < fwer_target / 2,\n                                  cdf_corr < fwer_target / 2)\n\n    maps = []\n    titles = []\n\n    maps.append(np.reshape(beta, shape))\n    titles.append('True weights')\n\n    maps.append(np.reshape(beta_extended, shape))\n    titles.append('True weights \\nwith tolerance')\n\n    maps.append(np.reshape(selected_dl, shape))\n    titles.append('Desparsified Lasso')\n\n    maps.append(None)\n    titles.append(None)\n\n    maps.append(np.reshape(selected_cdl, shape))\n    titles.append('CluDL')\n\n    maps.append(np.reshape(selected_ecdl, shape))\n    titles.append('EnCluDL')\n\n    plot(maps, titles)\n\n\nif __name__ == '__main__':\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}